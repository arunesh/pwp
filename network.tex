\section{Network Subsystem} 

The network layer is a fully decentralized, peer-peer overlay routing layer among the nodes participating in the Picolo
database network. The most important goal of the network layer is to help locate content as efficiently and quickly as
possible while surviving certain kinds of failure or malicious intent. Peer-peer networking literature refers to this
functionality as Decentralized Object Location and Routing (DOLR) \cite{dolr2003}. The network layer focuses on routing
messages, such as database queries, read/write requests or other management functions to the respective nodes which can
satisfy them. 

The overlay layer can be implemented on top of any datagram network protocol such as UDP or IP. Specifically on the Internet, we realize its implemented on IP or IPv6 protocols.
The peer-to-peer overlay routing infrastructure offers efficient, scalable, location-independent
routing of messages directly to nearby copies of an object or service using only localized resources [TP].

-- Self-repairing.
-- Soft-state based routing.

\subsection{Background}

In this Section, we provide a brief overview of peer-peer systems both in the research and product community that have
influenced the literature over the last 20 years.

Napster \cite{Napster} was one of the first popular service that provided much of the original inspiration for peer-peer
systems although its database was centralized.  DNS is an example of a widely deployed distributed and largely
decentralized key-value database that powers every lookup and interaction on the Internet \cite{Mockapetris_1988}. DNS
relies on special root servers to bootstrap the lookup protocol. The Freenet \cite{freenet_thesis, Clarke_2001} and the
Gnutella \cite{Gnutella} p2p systems were popular in the previous decade for file sharing. Both systems were designed
for sharing of large files over a longer duration of time. Content reliabilty including lookup reliability and network
latency goals were necessary in this enviroment. 

The second generation of peer-peer systems include research driven projects such as Chord \cite{Stoica_2001}, Content
Addressable Network (CAN)
\cite{Ratnasamy_2001}, Pastry \cite{Rowstron_2001}, Tapestry \cite{tapestry2004} and
Kademlia. Chord along with CAN, Tapestry and Pastry developed the concept of distributed hash tables (DHTs) as a
fundamental mechanism for content-based addressing. They built over the scalability and self-organizing properties of
both FreeNet and Gnutella by providing a definite answer to a lookup query in a bounded number of network hops. In fact,
these protocols are able to locate content within \( O(log N) \) where \(N\) is the number of nodes in the system. From
an 
API perspective, these overlays provide a key-based routing (KBR) interface that supports deterministic routing of
messages to a live node that has the responsiblity for the "value" corresponding to the given key. These systems also
support high level APIs such as Dynamic Object Location and Routing (DOLR) \cite{dolr2003}. Most DHT's use the concept
of consistent hashing to distribute the load evenly among the nodes.

{\em Consistent hashing:}
Typical hashing based schemes do a good job of spreading load through a known, fixed collection of servers. Since the
Blockchain consists of nodes on the Internet which can appear and disappear based on incentives and other criteria, our
assumption is that machines come and go as they crash or are brought into the network. Also,
the information about what nodes are functional propagates slowly through the
network, so that clients may have incompatible “views” of which nodes are available to replicate data. (Note that a
node can also be a client). This makes standard hashing useless since it relies on clients agreeing on which nodes are responsible for serving a particular
page.

Like most hashing schemes, consistent hashing assigns a set of items to buckets so that
each bin receives roughly the same number of items.  Unlike standard hashing schemes, a small change in the bucket set
does not induce a total remapping of items to buckets. In addition, hashing items into slightly different sets of
buckets gives only slightly different assignments of items to buckets. 

Chord uses such hash functions to map nodes and content uniformly to a circular 160-bit
namespace. Chord improves the scalability of consistent hashing by removing the requirement that every node knows about
every other node. Each node only maintains about \(O (log N) \) state information about other nodes in an \( N \) node
network. When nodes join or leave, they require \( O(log^2 N) \) messages to keep the network updated.

CAN routes messages in a {\em d}-dimensional space where each node maintains a routing table with \(O(d)\) entries and
any node can be reached in \(O(dN^{1/d})\) routing hops. CAN's routing table does not grow with network size, but the
number of routing hops grows faster than \(log N\).

When compared to Chord and CAN, Pastry and Tapestry take network distances into account when constructing overlay
topologies. While Chord and CAN use shortest overlay hops and other runtime heuristics, both Tapestry and Pastry
construct locally optimal routing tables to reduce any routing inefficiencies. 

Pastry and Tapestry share some similarities to the work by Plaxton et al \cite{Plaxton_1997} and to the routing layer in
the Landmark hierarchy \cite{Tsuchiya_1988}. The approach consists of routing based on address prefixes or otherwise
called prefix-based routing. However, both Pastry and Tapestry include an ability to self-organize the network structure
and achieve network locality in content mapping which also lends support for replication.
In addition, Tapestry also allows some application-based locality management by "publishing" location pointers
throughout the network for efficiently locating content and services.

Kademlia \cite{kademlia} is another p2p DHT-based routing system that uses prefix-based routing by arranging 160-bit IDs
(node IDs and content IDs) in a binary tree style data-structure for efficient routing. It uses an XOR-based distance
metric for building the routing table and for the routing algorithm itself. In terms of its performance and other
features, it is very similar to the above systems such as Chord, CAN, Pastry and Tapestry, but it offers simplicity in
its routing and lookup algorithms which make it attrative for implementation. IPFS \cite{ipfs} uses a version of
Kademlia for locating files for decentralized applications.

Other notable systems include Viceroy \cite{viceroy} which provides logarithmic hops through nodes with constant degree
routing tables. SkipNet \cite{skipnet} uses a multidimensional skip-list data structure to support overlay routing,
maintaining both a DNS-based namesapce for operational locality and a randomized namespace for network locality. Other
overlay proposals such as Koorde \cite{koorde} and Naor et al \cite{simple_hash} attain lower bounds on local routing
state but oversimplify some of the other features. 

The third generation of P2P research includes building applications on top of these DHT systems, validating them as
novel infrastructures or tuning them for specific use cases. For example, applications such as PAST \cite{past} and
SCRIBE \cite{scribe} are built on top of Pastry. Decentralized file storage application project OceanStore \cite{oceanstore} was built
on top of Tapestry, while CFS \cite{cfs} was build on top of Chord. FarSite \cite{farsite} uses a conventional
distributed directory service and could be built on top of Pastry. Another example of an overlay network is the Overcast
System \cite{overcast}, which provides reliable single-source multicast streams.

\subsection{Design of the Picolo overlay network}

The Picolo overlay network consists of a p2p DHT based lookup for mapping key to objects, content or services. There is a caching layer that allows for frequenly used items to be propagated closer to the demand endpoints in the network including ability to replicate as needed.

\subsubsection{Picolo namespace} 
1. Nodes and content map to a single 256-bit space. We can use existing hash functions such as SHA-256 for this purpose.

This space can be segregaged using a namespace identifier, which allows multiple such "naming layers" to co-exist. For example, one way to assign naming layers could be based on a per-application type.

For the rest of this section, the discussion gets isolated to within a single naming layer.

\subsubsection{Core API}
Picolo's network layer supports an API similar to a standard p2p overlay network, for a detailed discussion refer
\cite{dolr2003}. The primary goal of the API is to publish and locate objects or service identifiers within a given
namespace. All operations that occur in a namespace can be replicated across namespaces as needed.

Currenlty, we support the following API methods in a decentralized manner:
\begin{itemize}
    \item Publish()
    \item Unpublish()
    \item Lookup()
    \item RemoteCall()
\end{itemize}

\subsubsection{Routing and Lookup}
Explain how a DHT works. Add a layer that takes care of data locality

\subsection{Node Dynamics}

Failures, node departures, node additions.

\subsection{Replication and caching}
Beehive stuff for O(1) lookups for power law queries

Structured peer-peer distributed hash tables can implement lookups in \( O(log N)\) hops. Since each hop could add between 100-500
ms of latency, for a network of 10K to 100K nodes this computes to about 4-5 seconds for a lookup (or more). Thus, we
require a caching and replication strategy to reduce this cost especially for popular items. The discussion in this
Section applies to any content or service thats is made available through the p2p system and thus extends beyond the
Database as a blockchain service provided by Picolo.

There has been significant prior work on caching and replicating strategies for peer-peer overlay networks. Methods such
as Beehive \cite{beehive} provide a closed-form replication algorithm based on a derived equation that guarantees
\(O(1)\) lookup but require full knowledge of the network, such as number of nodes and popularity values for each data
item. While this system might be good for analysis and benchmark purposes, its implementation is not pratical for
Picolo. Kelips \cite{kelips} is a probabilistic algorithm that also provides \(O(1)\) lookup performance (in the
expected case) by dividing the network into \(O(\sqrt(N))\) affinity groups each of \(\sqrt(N)\) size. They use the
gossip protocol to replicate content to all nodes within an affinity group. Work by Gupta et al \cite{one_hop_lookup}
explore the tradeoff between routing table size and lookup latency. They offer a guaranteed \(O(1)\) lookup by
maintaining routes to each and every node in the network. Farsite \cite{farsite} provides a better ttradeoff by using
routing tables of \(O(dn^{1/3})\) size to route within \(O(d)\) hops.

Other peer-peer applications such as PAST \cite{past} and CFS \cite{cfs} use fixed size caches on intermediate nodes to
cache the objects being queried. While they are unable to provide closed-form analytic bounds on query time, their
average case performance is reasonably good.

We draw on the above literature to find the right balance between routing table size, cache and replication storage at
nodes versus lookup latency. The strategy presented in this Section achieves \(O(1)\) lookup on average for networks with standard node
and popularity dynamics. In other words, we allow for node joins, failures, unexpected departures (including malicious
intent) along with changes in service or object popularity. We also allow for "flash crowds", that is, an item, object
or service (such as a table-shard, or certain rows in a table) can quickly gain popularity (as given by standard
Internet virality models \cite{virality_model}).

\begin{itemize}
    \item A node caches or replicates an item with a probability that is proportional to the number of queries it is
        expected to get for that item in the upcoming time interval T.
    \item The difference between a cache and a replica is application specific. For the Database application that the
        network layer hosts, it depends on whether a particular table or a shard is allowed to have write permissions on
        the replica.
    \item When a node caches or replicates an item, it affects the probabilitic demand distribution for the item at
        nodes that are on the routing path which would have gotten the request has this item not been cached.
    \item Thus, by repeating this algorith iteratively, it would converge to the best caching pattern which would reduce
        lookup times with high probability for all items on the network.
\end{itemize}

\subsection{P2P Connectivity protocol}
Use gossip or some other protocol to maintain network topology, connections

\subsection{Cryptoeconomics and Byzantine behavior}
How will malicious nodes affect the system and how to mitigate/prevent/recover. Do nodes have incentive to participate in network discovery

\subsection{Analytics and Debug/Fault diagnosis}
