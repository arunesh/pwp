\section{Network Subsystem} 

The network layer is a fully decentralized, peer-peer overlay-based routing layer among the nodes participating in the
Picolo database network. The most important goal of the network layer is to help locate content as efficiently and
quickly as possible while surviving certain kinds of failure or malicious intent.

The network layer focuses on delivering messages which have application semantics such as database queries, read/write
requests or other management functions to the respective nodes which can satisfy them. In the rest of this Section, we
use the word object or content to loosely refer to content such as Tables, Shards, Metadata and such, i.e., any
application specific data that is content-addressable and needs to be part of the lookup layer. An item is content
addressable if its {\em address} or {\em key} or the primary {\em identifier} is purely a function of its content and
does not include parameters relating to the location and method of storage. Our p2p overlay routing infrastructure offers
efficient, scalable, location-independent routing of messages directly to nearby copies of an object or service using
only localized resources.

{\em Location Independent routing:} Location independent routing refers to a class of techniques for locating objects
based on content rather than their location, while attempting to find the shortest path possible to reach such objects.
This property is important in our design for Picolo and any decentralized system since it helps reduce the impact of
failures, node departures and Byzantine node behavior (including collusion) since a node does not have explicit {\em
control} over the content it can host or be responbile for. This control gets delegated to the content routing layer,
where using certain mechanisms, it becomes possible to provide some level of hosting/location control to the application
(dapp) developer, user or other policy-based decision processes (such as privacy policies enforced by a Government or
organization).  Peer-peer networking literature refers to this functionality as Decentralized Object Location and
Routing (DOLR) \cite{dolr2003}.

Picolo's design offers the following properties which are required of any high
performance p2p overlay-based lookup layer:

\begin{itemize}
    \item {\em Determinstic Node Mapping}: Picolo is able to locate objects anywhere in the network. That is, there
        should be no object in the network that cannot be accessed using the lookup layer. Also, the mapping of objects
        to their location in the network should be the same regardless of where the lookup originates.
    \item {\em Low routing inefficiency}: Routes should have low {\em stretch}. Stretch is the ratio between (network
        level) distance traveled by a query to an object and the minimal network distance from the query to the object.
        Optimal solution would be to always send the query to the nearest copy possible.
    \item {\em Balanced load}: The load should be evenly distributed across the nodes in the network, thus, reducing
        hotspots.
    \item {\em Dynamic membership}: The system allows arrival and departure of nodes while maintaining functionality.
        This is important for handling failures and Byzantine behavior of nodes.
\end{itemize}

In the next subsection \ref{net:background}, we summarize a wide-range of research on p2p overlays and storage systems over the last two
decades that has influenced our design and thought process in Picolo. In Section \ref{net:design} we present the overall
design of the network layer including the namespace (Section \ref{net:namespace}), 

This overlay layer can be implemented on top of any datagram network protocol such as UDP or IP for transport. While we
use IP or IPv6 for the layer-3 connectivity on the Internet, it is possible to use a suite of protocols for the
transport layer depending on the localized network conditions. In Section \ref{net:net_proto}, we discuss the details of
the transport mechanism used in Picolo for node-node communication.

\subsection{Background} \label{net:background}

Napster \cite{Napster} was one of the first popular service that provided much of the original inspiration for peer-peer
systems although its database was centralized.  DNS is an example of a widely deployed distributed and largely
decentralized key-value database that powers every lookup and interaction on the Internet \cite{Mockapetris_1988}. DNS
relies on special root servers to bootstrap the lookup protocol. The Freenet \cite{freenet_thesis, Clarke_2001} and the
Gnutella \cite{Gnutella} p2p systems were popular in the previous decade for file sharing. Both systems were designed
for sharing of large files over a longer duration of time. Content reliabilty including lookup reliability and network
latency goals were necessary in this enviroment. 

The second generation of peer-peer systems include research driven projects such as Chord \cite{Stoica_2001}, Content
Addressable Network (CAN)
\cite{Ratnasamy_2001}, Pastry \cite{Rowstron_2001}, Tapestry \cite{tapestry2004} and
Kademlia. Chord along with CAN, Tapestry and Pastry developed the concept of distributed hash tables (DHTs) as a
fundamental mechanism for content-based addressing. They built over the scalability and self-organizing properties of
both FreeNet and Gnutella by providing a definite answer to a lookup query in a bounded number of network hops. In fact,
these protocols are able to locate content within \( O(log N) \) where \(N\) is the number of nodes in the system. From
an 
API perspective, these overlays provide a key-based routing (KBR) interface that supports deterministic routing of
messages to a live node that has the responsiblity for the "value" corresponding to the given key. These systems also
support high level APIs such as Dynamic Object Location and Routing (DOLR) \cite{dolr2003}. Most DHT's use the concept
of consistent hashing to distribute the load evenly among the nodes. Consistent hashing refers to a technique which
creates `buckets' of items such that a small change in the buckets does not lead to a large amount of rehashing.

% {\em Consistent hashing:}
% Typical hashing based schemes do a good job of spreading load through a known, fixed collection of servers. Since the
% Blockchain consists of nodes on the Internet which can appear and disappear based on incentives and other criteria, our
% assumption is that machines come and go as they crash or are brought into the network. Also,
% the information about what nodes are functional propagates slowly through the
% network, so that clients may have incompatible “views” of which nodes are available to replicate data. (Note that a
% node can also be a client). This makes standard hashing useless since it relies on clients agreeing on which nodes are responsible for serving a particular
% page.
% 
% Like most hashing schemes, consistent hashing assigns a set of items to buckets so that
% each bin receives roughly the same number of items.  Unlike standard hashing schemes, a small change in the bucket set
% does not induce a total remapping of items to buckets. In addition, hashing items into slightly different sets of
% buckets gives only slightly different assignments of items to buckets. 
% 
Chord uses such hash functions to map nodes and content uniformly to a circular 160-bit
namespace. Chord improves the scalability of consistent hashing by removing the requirement that every node knows about
every other node. Each node only maintains about \(O (log N) \) state information about other nodes in an \( N \) node
network. When nodes join or leave, they require \( O(log^2 N) \) messages to keep the network updated.

CAN routes messages in a {\em d}-dimensional space where each node maintains a routing table with \(O(d)\) entries and
any node can be reached in \(O(dN^{1/d})\) routing hops. CAN's routing table does not grow with network size, but the
number of routing hops grows faster than \(log N\).

When compared to Chord and CAN, Pastry and Tapestry take network distances into account when constructing overlay
topologies. While Chord and CAN use shortest overlay hops and other runtime heuristics, both Tapestry and Pastry
construct locally optimal routing tables to reduce any routing inefficiencies. 

Pastry and Tapestry share some similarities to the work by Plaxton et al \cite{Plaxton_1997} and to the routing layer in
the Landmark hierarchy \cite{Tsuchiya_1988}. The approach consists of routing based on address prefixes or otherwise
called prefix-based routing. However, both Pastry and Tapestry include an ability to self-organize the network structure
and achieve network locality in content mapping which also lends support for replication.
In addition, Tapestry also allows some application-based locality management by "publishing" location pointers
throughout the network for efficiently locating content and services.

Kademlia \cite{kademlia} is another p2p DHT-based routing system that uses prefix-based routing by arranging 160-bit IDs
(node IDs and content IDs) in a binary tree style data-structure for efficient routing. It uses an XOR-based distance
metric for building the routing table and for the routing algorithm itself. In terms of its performance and other
features, it is very similar to the above systems such as Chord, CAN, Pastry and Tapestry, but it offers simplicity in
its routing and lookup algorithms which make it attrative for implementation. IPFS \cite{ipfs} uses a version of
Kademlia for locating files for decentralized applications.

Other notable systems include Viceroy \cite{viceroy} which provides logarithmic hops through nodes with constant degree
routing tables. SkipNet \cite{skipnet} uses a multidimensional skip-list data structure to support overlay routing,
maintaining both a DNS-based namesapce for operational locality and a randomized namespace for network locality. Other
overlay proposals such as Koorde \cite{koorde} and Naor et al \cite{simple_hash} attain lower bounds on local routing
state but oversimplify some of the other features. 

The third generation of P2P research includes building applications on top of these DHT systems, validating them as
novel infrastructures or tuning them for specific use cases. For example, applications such as PAST \cite{past} and
SCRIBE \cite{scribe} are built on top of Pastry. Decentralized file storage application project OceanStore \cite{oceanstore} was built
on top of Tapestry, while CFS \cite{cfs} was build on top of Chord. FarSite \cite{farsite} uses a conventional
distributed directory service and could be built on top of Pastry. Another example of an overlay network is the Overcast
System \cite{overcast}, which provides reliable single-source multicast streams.

\subsection{Design of the Picolo overlay network}
\label{net:design}

The Picolo overlay network consists of a p2p DHT based lookup for mapping key to objects, content or services. This
mapping resides inside a namespace, thus funtionality gets replicated across namespaces allowing a wide-range of
applications to co-exist. The APIs provided by the network layer are influenced by research from the P2p networking
community and discussed in Section \ref{net:core_api}.  There is a caching layer that allows for frequenly used items to
be propagated closer to the demand endpoints in the network including ability to replicate as needed.

A Picolo {\em network node} is a node that participates in the Picolo network overlay functionality such as primary and
secondary routing, functions that allow topology maintenance and related mechanisms. It is possible for a Picolo node to
particpate in the network functionality, the database functionality or both depending on its resources (such as disk
space, internet connectivity).

\subsubsection{Picolo namespace} 
\label{net:namespace}

Both nodes and content have IDs that map to a single 256-bit space using a hash function. This technique is fundamental
to make the routing layer content-addressable. The addressing is hexadecimal based, which is the radix used for the
prefix calculation and lookup in the routing table. This design is similar to how addressing and routing work in IPv6.
The node ID is a hash of its public-key certificate using a SHA-256 hash function. The content ID is a SHA-256 hash of
the application type, database name and version and the table name (and possibly a shard number if needed). As long as
the same consistent method is used to create content IDs, the actual format of the input string doesn't affect
functionality at the network layer (and can thus be determined by the application being used such as a Database, etc).

This space can be segregaged using a namespace identifier, which allows multiple such "naming layers" to co-exist. For
example, one way to assign naming layers could be based on a per-application type. The content and node IDs have to be
unique only within the namespace. The default namespace, which is global, is the keyword "default".

Each namespace has a separate datastructure and functionality thus, creating an isolated "virtual overlay" on a
per-namespace basis. For the rest of this section, the discussion gets isolated to within a single naming layer, such as
the "default" namespace.

\subsubsection{Core API}
\label{net:core_api}
Picolo's network layer supports an API similar to a standard p2p overlay network, for a detailed discussion refer
\cite{dolr2003}. The primary goal of the API is to publish and locate objects, content or service identifiers within a given
namespace. All operations that occur in a namespace can be replicated across namespaces as needed.

Currenlty, we support the following API methods in a decentralized manner:
\begin{itemize}
    \item {\em Publish(namespace, content key, object, credentials)}: Makes the content available on the network, including first a copy on
        the local node. The object could be a database table or a shard. Operations on the object fall within the
        context of the database layer and are discussed in a later Section.
    \item {\em Unpublish(namespace, content key, credentials)}: Removes the content from the network making it inaccessible.
    \item {\em Lookup(namespace, content key, credentials)}: Lookup item with the given content key using the p2p lookup protocol
        discussed in this Section. The API returns a node identifier which can be used in the next method to execute a
        remote funationality on it.
    \item {\em RemoteCall(namespace, node ID or content key, credentials, method details)}: This executes a given functionality
        on a remote node. If the key is given, it first does a node lookup using the Lookup() method.
\end{itemize}

These methods can be called on a single node. They can also be called using a client SDK which shall be provided. The
client SDK would use the DHT layer to first find "any" node, connect to it and call the above methods. These methods
might use other algorithmic constructs which define core aspects of the routing layer and shall be discussed further in
the following subsections.

\subsubsection{Routing and Lookup}
\label{net:routing}

Picolo uses a prefix-based routing mechanism to find the node for a given content key. The content key can be computed
by using a combination of the Database name, Table name and shard ID (if sharding exists, or version information).
Optional values such as read/write shards or specific functionality flavors can also be used to "color" the content key
(i.e. create variants or replicas etc).

% TODO Figures:
% network stretch example (nice to have).

% TODO:
% DONE. Describe the routing map. level by level. for a single lookup node. Diagram to illustrate an example routing map.
% Describe the home node function as hash(content_ID, j) for j = 1 to R_N to generate N home nodes. ContentID = a json
% blob of a 256 bit string and some parameters such as replica size R_N.
% RouteToDelegates(home_node), describe delegate based routing. Example using a diagram.
% Describe algorithm to construct the neighbor map for a new node. any optimizations.
%   Describe algorithm to notify neighbors.
% Discussion on failures and any optimizations.

{\em Routing to a destination ID:}
Given a destination ID, each Picolo node that participates in the routing functionality constructs a routing map, which
is similar to routing tables used in layer-3 systems today. Using these routing maps, routing of a message happens {\em
locally} and {\em incrementally} at each node in the overlay path to the destination. The core routing method is to
route using a prefix-match. This is similar to the Classless InterDomain Routing (CIDR) routing architecture for the Internet
\cite{cidr_rfc}.

The routing works as follows. We resolve digits from the most significant to the least (left to right). A node N has a
routing map with multiple levels. The number of levels are equal to the number of digits in the ID space. For a 256 bit
ID with a hexadecimal base, we have 64 digits. At level \(i\), we have nodes that match \(i\) digits with the
destination ID for \( i \in [ 0..63 ]\).

At any given level, the number of entries are equal to the base of the ID space, that is, we have 16 entries, one for
each digit \( [ 0, 1, 2..F ] \). At the \(i\) th level, the \( j\)th entry is the node with a prefix of \( prefix(ID, i)
+ j\), where \(prefix(A,j\) is represents the prefix in string \(A\) of length \(j\). As an example, Table
\ref{table:routing_map} shows the different levels for the node with ID "691D".  Suppose we wish to route the ID "692B"
given the routing table and we are doing a lookup at level 2. This means we have a prefix match of "69" and are
searching for a node in the column for the digit "2". This happens to be node \(C_2\) and thus the query is then
forwarded there. Similarly, suppose we are asked to route to node "615B". This would be at level 1 since we have a match
for the prefix "6". We would thus route to node \(B_1\) matching the column"1" and row "6".

\begin{table}[h]
\begin{center}
    \renewcommand{\arraystretch}{1.5}
\begin{tabular} {| c | c | c | c | c | c |}
    \hline
    Prefix & 0 & 1 & 2 & .. & F \\
    \hline
    \hline
    None & \(A_0\) & \(A_1\) & \(A_2\) & .. & \(A_F\) \\
    6 & \(B_0\) & \(B_1\) & \(B_2\) & .. & \(B_F\) \\
    69 & \(C_0\) & \(C_1\) & \(C_2\) & .. & \(C_F\) \\
    691 & \(D_0\) & \(D_1\) & \(D_2\) & .. & \(D_F\) \\
    \hline
\end{tabular}
\label{table:routing_map}
\caption{Example of a routing map for node with ID "691D".}
\end{center}
\end{table}

{\em Home Node Set:} The content ID for a given content is computed using a cryptographic function such as SHA-256. The
network design allows the use of any cryptographic hash function with a even distribution of hash values. The content ID 
consists of this hash value, a {\em replica count r} and possibly any other parameters bundled as a json blob.

As discussed in the previous Section both node IDs and content IDs are confined to a 256-bit space.  Every content ID
maps to one or more {\em home nodes} that form a {\em home node set}. A home node is the node that is responsible for
either directly hosting the content if it has that capability or has a link to another node that hosts content.
Essentially from the routing layer's perspetive, the routing function ends once we reach a home node. If the replica
count \( r > 1\), then the content maps to a {\em set} of home nodes where each home node is independent from the rest.

The function \(HomeNodes(ContentID)\) returns a set of home nodes. An implementation of this function using SHA-256 is as follows:
\begin{align}
    X_id &= \{X_{256}, r\} \\
    H_X &= HomeNodes(X_id) \\
        &= \{ y_i : y_i = SHA-256(X_{256}, j) \forall j = 1 .. r\}
\end{align}

Mapping a content to multiple home nodes is purely for fault tolerance purposes and can be configured by the Database
schema (application designer) as needed. For simplicity of the rest of this Section, we shall assume that there is a
single home node, i.e., \( r = 1\) as all the algorithms and concepts easily extend to the case with multiple home
nodes.

In Picolo, nodes can participate in different kinds of functionality such as hosting or routing or both. This
segregation allows certain nodes to either participate in the network routing functionality (and handle that load) or
provide the database functionality or both depending on various factors (such as resource availability). In the case
that a Picolo home node stores a link to another node which hosts the content, such a link is called a {\em content
link}. This link is not part of the routing layer since the routing functionality ends with finding a home node. When a
Picolo node stores a link to another node as a part of the routing map such a link is called a {\em node link}.

The goal of the routing protocol is to find a route with low stretch (ideally a stretch of 1.0). Stretch is the ratio
between (network level) distance traveled from node A to node B to the minimal network distance between node A and B.
Since we are routing at the overlay layer, a route on the p2p overlay might result in a suboptimal route at the network
level. Figure XXX illustrates the concept of network stretch and why a low stretch is important. In essence, stretch
captures any inefficiencies at the network layer incurred by routing at the p2p overlay layer.

FILL

Picolo's network layer maps a given content ID to a set of "home" nodes. Let \(x\) be a node and \(H_x =
FindHomeNodes(x) \) be the set of home nodes for \(x\). If \( |H_x| = 1\) then each content ID has exactly one home
node. It might be desirable to map a given content ID to more than 1 nodes for application specific needs such as a
table-shard replication. In any case, the size of \( H_x \) should be small for any given \(x\).

\begin{property}[Unique Home node set] The home node set \(H_x\) for a content ID \(x\) must be unique, that is,
    \(H_x\) must generated the exact same set regardless of where it is computed in the network.
\label{property_unique_home}
\end{property}

Network nodes in Picolo have two types of routing links. The first is called a {\em principal} route which defines the
core neighbor links that connect the node to its neighbors. Issues with these links might cause the node to be marked as
having failures (and thus trigger failure mode operation semantics). The second set of links are used for optimizations,
such as to content links to popular items, or to cache links to popular home nodes etc. We call these the {\em cache
links} or {\em secondary links}.

Nodes in Picolo send periodic {\em heartbeat} messages to other nodes through the p2p overlay network as given by their
routing tables. This heatbeat message includes any content links or content IDs that the nodes are hosting. These messages
are routed along the {\em principal} routing links which every node maintains. These messages help refresh the primary
link based routing overlay network and help discover "holes" or link/node failures. The messages also help deposit
content links at the nodes that forward the heartbeats. The content links expire according to a timeout and are
refreshed with such messages. This operation of advertising content is called {\em publishing}. Picolo also supports
targeted republishing of content such as when a node comes back online after a disruption or when a node leaves/joins
the network.

{\em Query routing:}: Lookup or querying for a specific content ID (or a set) happens in the following manner. A client
that wishes to make such a query connects to one of the network nodes in Picolo. The network node then computes
\(FindHomeNodes(contentID)\) for the requested content. The lookup or query request is then forwarded to one (or more)
of the home nodes from this set. In the process of this lookup, if at any stage, multiple node paths are encountered,
then the query can be forwarded along all paths (to minimize latency but incur a slightly higher cost) or can be
forwarded along the path with the best latency. These tradeoffs can be exposed at the \(FindHomeNodes(contentID)\) API
call to allow the application developer to make such choices.

FILL

Figure XXX shows an example of routing a query for a content ID YYY. The dotted arrows show the cached content links from
secondary or cache routing network. The primary or principal links are shown in solid arrow. The example the lookup operation
for the content ID YYY originating at two different points in the network A and B. The query at A utilizes the principal
links and finds the home node for the content ID YYY. The query at B ends up hitting a node C which has a cached link that
directly routes it to the home node for YYY, thus saving a few network hops. In practice, our routing caching algorithm will
ensure that most lookups and queries can be satisfied using the cache routing network, so that we can get an \(O(1)\) lookup
performance in the expected/average case.

\begin{theorem}
Picolo's network layer can perform location independent routing given Property \ref{property_unique_home}.
\end{theorem}

The proof for this theorem follows from two facts. First, for a content \(x\), the function \(H_x = FindHomeNodes(x)\) returns
the same set of nodes regardless of which network node this is executed on, thus, proving that the result is purely a function
of the content \(x\). Secondly, due to the routing protcol requires home nodes (either directly hosting content or content links)
to periodically send heartbeat messages through the principal links to the rest of the network. This ensures that any node in the 
network will have the routing layer capabilty to reach atleast one node in this set \(H_x\), thus demonstrating the invariant. 

\begin{theorem}[Fault Tolerance]
   
    For a content ID \(x\) with its set of home nodes given by \( H_x = FindHomeNodes(x) \) and \(S_n\) be the set of
all nodes (of cardinality \(n\)) which satisfies the above properties, if \( |H_x| > 1 \) and \( \forall a, b \in S_n,
    P(a|b) = P(a) \) where \( P(a) = \) probability of node \(a \in H_x \), then \(H_x\) has fault tolerance with
    relabilty of \( | H_x | \).
\end{theorem}

\subsubsection{Delegate Routing}

Node and content IDs in Picolo map to a relatively large space, i.e. \(2^{256}\) in size. Let \( H_x =
FindHomeNodes(x)\) for content ID x. Given the large ID space and relatively sparse nature of the network nodes (a
thousand or a million nodes out of \(2^{256}\), it is unlikely that nodes in \(H_x\) would have an exact match in the
network. This is the case with all other content based DHTs as well \cite{Stoica_2001, Rowstron_2001, tapestry2004}.
Nevertheless, we route to each node in \(H_x\) as if it exists. During this process we might encounter empty neighbor
links on the way. In such cases, the algorithm selects an alternate link but does it in a way that is deterministic
across the whole network. Routing will terminate when we reach a node which is the best match and the lookup cannot be
routed further. This node then, becomes the {\em Delegate} for the subject node \(y\). We call this {\em Delegate
Routing} where we route the query to each node \(y \in H_x \) such that it gets mapped to a node that is a {\em
delegate} for \(y\).

In the routing table, an entry at a level \(l\) will be empty only if no such nodes exist in the network that have a
match. Thus, the same entry will be empty for all nodes in the network. As a result, regardless of where the lookup
query originates for a content ID \(x\), it will always compute to the same delegate node \(y\). Delegate routing thus
becomes a determinstic routing function computed by the network in a distributed manner. It is important to note that
this mapping is the same regardless of where the function computation originates in the network.
% TODO mark this as a property ?
% We solve this problem by mapping each node \(y \in H_x \) to a \(Delegate(y)\) which is a node that exists on the network.

\subsection{Node Dynamics}
\label{net:node_dynamics}

{\em Node insertion:} Node insertion happens in the following manner. A new node \(N\) first creates its 256 bit ID,
say, \(N_{id}\), using any credentials it has. Credentials can include node identifier information, owernship certificate information and
such. Next, \( N \) connects to the core nodes that are available through DNS mechanisms. It uses a special API call
requesting that \(N\) be inserted into the network. \(N\) computes its neighbor maps in the following manner. It uses
the Delegate routing function to find the set of delegate nodes to route to the content ID \(N_{id}\). However, at each
hop we use the information from the nodes being contacted to construct the neighbor map. Finally, we inform the delegate
nodes of the new node \(N\), so that they can update their neighbor maps as well thus, ensuring that any future queries
for \(N_{id}\) will get routed to \(N\). 

{\em Failures and node departures:} Nodes can depart the Picolo overlay network in two ways: First, is the case of a
graceful departure, where the departing node send out an explicit notification through the network informing of the
departure. This allows the network to adapt before the node leaves, by updating the routing tables and moving content
out of the departing node if necessary.

Second is the case of a sudden departure such as a node getting shutdown, failing for various reasons or getting
disconnected from the network due to Internet connectivity issues. A node with a degraded Internet connection can also
be treated under this scenario. The basic mechanism to recover from such failures is using the heartbeat advertisements
as discussed previously. Each link on the overlay network includes heartbeat messages which help update the latency
metrics and also result in re-publishing or updating the content list hosted on a node. Upon missing a certain number of
messages from a node suspected of failure, neighboring nodes and request an explicit update from such a node. Upon
timeout, the neighboring nodes arrive at a consensus that the suspected node has failed and thus update their routing
tables accordingly by initiating the departure API calls as for the first case.

\subsection{Replication and caching}
\label{net:replication}
Structured peer-peer distributed hash tables can implement lookups in \( O(log N)\) hops. Since each hop could add between 100-500
ms of latency, for a network of 10K to 100K nodes this computes to about 4-5 seconds for a lookup (or more). Thus, we
require a caching and replication strategy to reduce this cost especially for popular items. The discussion in this
Section applies to any content or service thats is made available through the p2p system and thus extends beyond the
Database as a blockchain service provided by Picolo.

There has been significant prior work on caching and replicating strategies for peer-peer overlay networks. Methods such
as Beehive \cite{beehive} provide a closed-form replication algorithm based on a derived equation that guarantees
\(O(1)\) lookup but require full knowledge of the network, such as number of nodes and popularity values for each data
item. While this system might be good for analysis and benchmark purposes, its implementation is not pratical for
Picolo. Kelips \cite{kelips} is a probabilistic algorithm that also provides \(O(1)\) lookup performance (in the
expected case) by dividing the network into \(O(\sqrt(N))\) affinity groups each of \(\sqrt(N)\) size. They use the
gossip protocol to replicate content to all nodes within an affinity group. Work by Gupta et al \cite{one_hop_lookup}
explore the tradeoff between routing table size and lookup latency. They offer a guaranteed \(O(1)\) lookup by
maintaining routes to each and every node in the network. Farsite \cite{farsite} provides a better ttradeoff by using
routing tables of \(O(dn^{1/3})\) size to route within \(O(d)\) hops.

Other peer-peer applications such as PAST \cite{past} and CFS \cite{cfs} use fixed size caches on intermediate nodes to
cache the objects being queried. While they are unable to provide closed-form analytic bounds on query time, their
average case performance is reasonably good.

We draw on the above literature to find the right balance between routing table size, cache and replication storage at
nodes versus lookup latency. The strategy presented in this Section achieves \(O(1)\) lookup on average for networks with standard node
and popularity dynamics. In other words, we allow for node joins, failures, unexpected departures (including malicious
intent) along with changes in service or object popularity. We also allow for "flash crowds", that is, an item, object
or service (such as a table-shard, or certain rows in a table) can quickly gain popularity (as given by standard
Internet virality models \cite{virality_model}).

\begin{itemize}
    \item A node caches or replicates an item with a probability that is proportional to the number of queries it is
        expected to get for that item in the upcoming time interval T.
    \item The difference between a cache and a replica is application specific. For the Database application that the
        network layer hosts, it depends on whether a particular table or a shard is allowed to have write permissions on
        the replica.
    \item When a node caches or replicates an item, it affects the probabilitic demand distribution for the item at
        nodes that are on the routing path which would have gotten the request has this item not been cached.
    \item Thus, by repeating this algorithm iteratively, it would converge to the best caching pattern which would reduce
        lookup times with high probability for all items on the network.
\end{itemize}

\subsection{Node Connectivity protocol}
\label{net:net_proto}

We describe the details of the transport protocol layer of a Picolo node. We call this the Node Connectivity Protocol
layer. This protocol layer is responsible for connecting to a set of peers, maintaining network connectivity under
possibly varying network conditions and allowing for a peer-peer network programing model (as opposed to client-server
programming model which most transport protocols are based off).  The protocol layer should also work with nodes that
are behind Network Address Translators (NATs) \cite{nats}.

When a Picolo node starts for the first time (fresh install), it will query a set of "root" servers, similar to the DNS
architecture of the Internet which is one of the largest decentralized lookup databases in the world \cite{icann_root}. 
These root servers populate the nodes with a list of neighboring nodes and content to bootstrap with. It uses algorithms
outlined in the previous Section to populate its routing table.

As part of downloading the node binary, each download is given a certificate signed by the Picolo root servers. This
certificate is the seed credential that lets the node become part of the network and is presented to other nodes when it
wishes to be part of the core routing layer.

With respect to the network layer semantics for making connections, we shall use either the QUIC protocol or the SPDY
protocol.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/pic_netlayer.png}
  \caption{Comparison of network stacks for Traditional HTTP/2, QUIC and SPDY.}
\end{figure}

QUIC is a relatively new transport protocol designed by Google \cite{quic_sigcomm} that provides encrypted, multi-session transport over UDP
instead of TCP for HTTPS traffic. It replaces the traditional HTTPS stack: HTTP/2, TLS, TCP with an integrated protocol
layer directly over UDP. The key features include reduction in the head-of-line blocking delay and handshake delays to
improve the performance for multi-session connections. However, QUIC is relatively new and the knee-jerk reaction from
firewall manufacturers has been to block it until its fully understood. Also it does not work well with NATs that
provide STUN protocol \cite{stun_protocol} support for drilling holes through their firewalls.  

The network layer would use techniques derived from the STUN protocol to traverse nodes
that are behind a NAT. By having a set of nodes (such as ones with a higher stake) behave as
super-peers, its possible to craft a p2p version of the STUN protocol to enable
other nodes behind a NAT to become part of the network \cite{p2p_nat}.

SPDY (pronounced "SPeeDY"), is a protocol from the Chromium Project at Google \cite{speedy_protocol} to improve network performance for web
pages. It requires changes at both end points for the protocol to work and is able to improve load times by about 64\%
on average. It build upon previous proposals such as the Stream Control Transmission Protocol (SCTP) \cite{sctp}
\cite{sctp_rfc} that allow multiple sessions to coexist on a single TCP connection. SPDY can work well in network
environments where firewalls are biased towards blocking most ports but the HTTP and HTTPS. SPDY allows bi-directional
session initiation to happen and thus is more p2p flavored that other web protocols. It also has multi-session support
over an SSL based secure layer that can utilize certificates as credentials among other methods.

%\subsection{Cryptoeconomics and Byzantine behavior}
%\label{net:crypto}
%How will malicious nodes affect the system and how to mitigate/prevent/recover. Do nodes have incentive to participate in network discovery

\subsection{Analytics and Debug/Fault diagnosis}
\label{net:analytics}

Nodes maintain local analytic counters which are periodically updated to a special distributed system-level analytics
database. Analytics include counters for table queries, statistics for various operations (CRUD), latency numbers, route
changes and similar network statistics. The analytics database is a special permissioned database that only entities
(developers, institutions) with the right credentials can access.

A similar but separate database is used for fault diagnosis, collecting data on Byzantine failures, crash logs and
unexpected node departures. 
