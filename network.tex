\section{Network Subsystem} 

The network layer is a fully decentralized, peer-peer overlay routing layer among the nodes participating in the Picolo
database network. The most important goal of the network layer is to help locate content as efficiently and quickly as
possible while surviving certain kinds of failure or malicious intent. Peer-peer networking literature refers to this
functionality as Decentralized Object Location and Routing (DOLR) \cite{dolr2003}. The network layer focuses on routing
messages, such as database queries, read/write requests or other management functions to the respective nodes which can
satisfy them. 

The overlay layer can be implemented on top of any datagram network protocol such as UDP or IP. Specifically on the
Internet, we realize its implemented on IP or IPv6 protocols.  The peer-to-peer overlay routing infrastructure offers
efficient, scalable, location-independent routing of messages directly to nearby copies of an object or service using
only localized resources \cite{tapestry2004}. We use the word object to loosely refer to content such as Tables, Shards,
Metadatd and such, that is, any content that is content-addressable and needs to be part of the lookup layer.  Location
independent routing refers to a class of techniques for locating objects based on content rather than their location,
while attempting to find the shortest path possible to reach such objects. Picolo's design offers the following
properties which are required of any high performance p2p overlay-based lookup layer:

\begin{itemize}
    \item {\em Determinstic Node Mapping}: Picolo is able to locate objects anywhere in the network. That is, there
        should be no object in the network that cannot be accessed using the lookup layer.
    \item {\em Low routing inefficiency}: Routes should have low {\em stretch}. Stretch is the ratio between (network
        level) distance traveled by a query to an object and the minimal network distance from the query to the object.
        Optimal solution would be to always send the query to the nearest copy possible.
    \item {\em Balanced load}: The load should be evenly distributed across the nodes in the network, thus, reducing
        hotspots.
    \item {\em Dynamic membership}: The system allows arrival and departure of nodes while maintaining functionality.
\end{itemize}

In the next subsection \ref{net:background}, we summarize a wide-range of research on p2p overlays and storage systems over the last two
decades that has influenced our design and thought process in Picolo. In Section \ref{net:design} we present the overall
design of the network layer including the namespace (Section \ref{net:namespace}), 

\subsection{Background} \label{net:background}

In this Section, we provide a brief overview of peer-peer systems both in the research and product community that have
influenced the literature over the last 20 years.

Napster \cite{Napster} was one of the first popular service that provided much of the original inspiration for peer-peer
systems although its database was centralized.  DNS is an example of a widely deployed distributed and largely
decentralized key-value database that powers every lookup and interaction on the Internet \cite{Mockapetris_1988}. DNS
relies on special root servers to bootstrap the lookup protocol. The Freenet \cite{freenet_thesis, Clarke_2001} and the
Gnutella \cite{Gnutella} p2p systems were popular in the previous decade for file sharing. Both systems were designed
for sharing of large files over a longer duration of time. Content reliabilty including lookup reliability and network
latency goals were necessary in this enviroment. 

The second generation of peer-peer systems include research driven projects such as Chord \cite{Stoica_2001}, Content
Addressable Network (CAN)
\cite{Ratnasamy_2001}, Pastry \cite{Rowstron_2001}, Tapestry \cite{tapestry2004} and
Kademlia. Chord along with CAN, Tapestry and Pastry developed the concept of distributed hash tables (DHTs) as a
fundamental mechanism for content-based addressing. They built over the scalability and self-organizing properties of
both FreeNet and Gnutella by providing a definite answer to a lookup query in a bounded number of network hops. In fact,
these protocols are able to locate content within \( O(log N) \) where \(N\) is the number of nodes in the system. From
an 
API perspective, these overlays provide a key-based routing (KBR) interface that supports deterministic routing of
messages to a live node that has the responsiblity for the "value" corresponding to the given key. These systems also
support high level APIs such as Dynamic Object Location and Routing (DOLR) \cite{dolr2003}. Most DHT's use the concept
of consistent hashing to distribute the load evenly among the nodes.

{\em Consistent hashing:}
Typical hashing based schemes do a good job of spreading load through a known, fixed collection of servers. Since the
Blockchain consists of nodes on the Internet which can appear and disappear based on incentives and other criteria, our
assumption is that machines come and go as they crash or are brought into the network. Also,
the information about what nodes are functional propagates slowly through the
network, so that clients may have incompatible “views” of which nodes are available to replicate data. (Note that a
node can also be a client). This makes standard hashing useless since it relies on clients agreeing on which nodes are responsible for serving a particular
page.

Like most hashing schemes, consistent hashing assigns a set of items to buckets so that
each bin receives roughly the same number of items.  Unlike standard hashing schemes, a small change in the bucket set
does not induce a total remapping of items to buckets. In addition, hashing items into slightly different sets of
buckets gives only slightly different assignments of items to buckets. 

Chord uses such hash functions to map nodes and content uniformly to a circular 160-bit
namespace. Chord improves the scalability of consistent hashing by removing the requirement that every node knows about
every other node. Each node only maintains about \(O (log N) \) state information about other nodes in an \( N \) node
network. When nodes join or leave, they require \( O(log^2 N) \) messages to keep the network updated.

CAN routes messages in a {\em d}-dimensional space where each node maintains a routing table with \(O(d)\) entries and
any node can be reached in \(O(dN^{1/d})\) routing hops. CAN's routing table does not grow with network size, but the
number of routing hops grows faster than \(log N\).

When compared to Chord and CAN, Pastry and Tapestry take network distances into account when constructing overlay
topologies. While Chord and CAN use shortest overlay hops and other runtime heuristics, both Tapestry and Pastry
construct locally optimal routing tables to reduce any routing inefficiencies. 

Pastry and Tapestry share some similarities to the work by Plaxton et al \cite{Plaxton_1997} and to the routing layer in
the Landmark hierarchy \cite{Tsuchiya_1988}. The approach consists of routing based on address prefixes or otherwise
called prefix-based routing. However, both Pastry and Tapestry include an ability to self-organize the network structure
and achieve network locality in content mapping which also lends support for replication.
In addition, Tapestry also allows some application-based locality management by "publishing" location pointers
throughout the network for efficiently locating content and services.

Kademlia \cite{kademlia} is another p2p DHT-based routing system that uses prefix-based routing by arranging 160-bit IDs
(node IDs and content IDs) in a binary tree style data-structure for efficient routing. It uses an XOR-based distance
metric for building the routing table and for the routing algorithm itself. In terms of its performance and other
features, it is very similar to the above systems such as Chord, CAN, Pastry and Tapestry, but it offers simplicity in
its routing and lookup algorithms which make it attrative for implementation. IPFS \cite{ipfs} uses a version of
Kademlia for locating files for decentralized applications.

Other notable systems include Viceroy \cite{viceroy} which provides logarithmic hops through nodes with constant degree
routing tables. SkipNet \cite{skipnet} uses a multidimensional skip-list data structure to support overlay routing,
maintaining both a DNS-based namesapce for operational locality and a randomized namespace for network locality. Other
overlay proposals such as Koorde \cite{koorde} and Naor et al \cite{simple_hash} attain lower bounds on local routing
state but oversimplify some of the other features. 

The third generation of P2P research includes building applications on top of these DHT systems, validating them as
novel infrastructures or tuning them for specific use cases. For example, applications such as PAST \cite{past} and
SCRIBE \cite{scribe} are built on top of Pastry. Decentralized file storage application project OceanStore \cite{oceanstore} was built
on top of Tapestry, while CFS \cite{cfs} was build on top of Chord. FarSite \cite{farsite} uses a conventional
distributed directory service and could be built on top of Pastry. Another example of an overlay network is the Overcast
System \cite{overcast}, which provides reliable single-source multicast streams.

\subsection{Design of the Picolo overlay network}
\label{net:design}

The Picolo overlay network consists of a p2p DHT based lookup for mapping key to objects, content or services. This
mapping resides inside a namespace, thus funtionality gets replicated across namespaces allowing a wide-range of
applications to co-exist. The APIs provided by the network layer are influenced by research from the P2p networking
community and discussed in Section \ref{net:core_api}.  There is a caching layer that allows for frequenly used items to
be propagated closer to the demand endpoints in the network including ability to replicate as needed.

\subsubsection{Picolo namespace} 
\label{net:namespace}

Nodes and content map to a single 256-bit space. The addressing is hexadecimal based, which is the radix used for the
prefix calculation and lookup in the routing table. This design is similar to both IPv6 and also used in Ethereum.  The
node ID is a hash of its public-key certificate using a SHA-256 hash function. The content ID is a SHA-256 hash of the
application type, database name and version and the table name (and possibly a shard number if needed). As long as the
same consistent method is used to create content IDs, the actual format of the input string doesn't affect functionality
at the network layer.

This space can be segregaged using a namespace identifier, which allows multiple such "naming layers" to co-exist. For
example, one way to assign naming layers could be based on a per-application type. The content and node IDs have to be
unique only within the namespace. The default namespace, which is global, is the keyword "default".

For the rest of this section, the discussion gets isolated to within a single naming layer, such as the "default"
namespace.

\subsubsection{Core API}
\label{net:core_api}
Picolo's network layer supports an API similar to a standard p2p overlay network, for a detailed discussion refer
\cite{dolr2003}. The primary goal of the API is to publish and locate objects, content or service identifiers within a given
namespace. All operations that occur in a namespace can be replicated across namespaces as needed.

Currenlty, we support the following API methods in a decentralized manner:
\begin{itemize}
    \item {\em Publish(namespace, content key, object, credentials)}: Makes the content available on the network, including first a copy on
        the local node. The object could be a database table or a shard. Operations on the object fall within the
        context of the database layer and are discussed in a later Section.
    \item {\em Unpublish(namespace, content key, credentials)}: Removes the content from the network making it inaccessible.
    \item {\em Lookup(namespace, content key, credentials)}: Lookup item with the given content key using the p2p lookup protocol
        discussed in this Section. The API returns a node identifier which can be used in the next method to execute a
        remote funationality on it.
    \item {\em RemoteCall(namespace, node ID or content key, credentials, method details)}: This executes a given functionality
        on a remote node. If the key is given, it first does a node lookup using the Lookup() method.
\end{itemize}

These methods can be called on a single node. They can also be called using a client SDK which shall be provided. The
client SDK would use the DHT layer to first find "any" node, connect to it and call the above methods.

\subsubsection{Routing and Lookup}
\label{net:routing}

Picolo uses a prefix-based routing mechanism to find the node for a given content key. The content key can be computed
by using a combination of the Database name, Table name and shard ID (if sharding exists). Optional values such as
read/write shards or specific functionality flavors can also be used to "color" the content key.

As discussed in the previous Section, using a DHT such as SHA-256, both node IDs and content IDs are now confined to a
256-bit space. Content keys map to node IDs that are the their "home" nodes. A home node is the one which has the
"closest" ID in terms of lexicographic distance in the ID space. The definition of is given by exeucting the
"FindHomeNodes(ContentID)" function on the network, as discussed later. The home node can either store the content itself or
a pointer to the node that stores the content. This segregation allows certain nodes to either participate in the
network routing functionality (and handle that load) or provide the database functionality or both depending on other
factors (such as resource availability).

The goal of the routing protocol is to find a route with low stretch (ideally a stretch of 1.0). Stretch is the ratio
between (network level) distance traveled by a query to an object and the minimal network distance from the query to the
object. Since we are routing at the overlay layer, a route on the p2p overlay might result in a suboptimal route at the
network level. Figure XXX illustrates the concept of network stretch and why a low stretch is important.

FILL

Picolo's network layer maps a given content ID to a set of "home" nodes. Let \(x\) be a node and \(H_x =
FindHomeNodes(x) \) be the set of home nodes for \(x\). If \( |H_x| = 1\) then each content ID has exactly one home
node. It might be desirable to map a given content ID to more than 1 nodes for application specific needs such as a
table-shard replication. In any case, the size of \( H_x \) should be small for any given \(x\).

\begin{property}[Unique Home node set]. The home node set \(H_x\) for a content ID \(x\) must be unique, that is,
    \(H_x\) must generated the exact same set regardless of where it is computed in the network.
\end{property}

\begin{itemize}
    \item Lookup using a prefix-based DHT. 
    \item Routing table description.
    \item Lookup algorithm.
    \item Caching links for faster lookup. 
\end{itemize}

\subsection{Node Dynamics}
\label{net:node_dynamics}

Failures, node departures, node additions.

\subsection{Replication and caching}
\label{net:replication}
Beehive stuff for O(1) lookups for power law queries

Structured peer-peer distributed hash tables can implement lookups in \( O(log N)\) hops. Since each hop could add between 100-500
ms of latency, for a network of 10K to 100K nodes this computes to about 4-5 seconds for a lookup (or more). Thus, we
require a caching and replication strategy to reduce this cost especially for popular items. The discussion in this
Section applies to any content or service thats is made available through the p2p system and thus extends beyond the
Database as a blockchain service provided by Picolo.

There has been significant prior work on caching and replicating strategies for peer-peer overlay networks. Methods such
as Beehive \cite{beehive} provide a closed-form replication algorithm based on a derived equation that guarantees
\(O(1)\) lookup but require full knowledge of the network, such as number of nodes and popularity values for each data
item. While this system might be good for analysis and benchmark purposes, its implementation is not pratical for
Picolo. Kelips \cite{kelips} is a probabilistic algorithm that also provides \(O(1)\) lookup performance (in the
expected case) by dividing the network into \(O(\sqrt(N))\) affinity groups each of \(\sqrt(N)\) size. They use the
gossip protocol to replicate content to all nodes within an affinity group. Work by Gupta et al \cite{one_hop_lookup}
explore the tradeoff between routing table size and lookup latency. They offer a guaranteed \(O(1)\) lookup by
maintaining routes to each and every node in the network. Farsite \cite{farsite} provides a better ttradeoff by using
routing tables of \(O(dn^{1/3})\) size to route within \(O(d)\) hops.

Other peer-peer applications such as PAST \cite{past} and CFS \cite{cfs} use fixed size caches on intermediate nodes to
cache the objects being queried. While they are unable to provide closed-form analytic bounds on query time, their
average case performance is reasonably good.

We draw on the above literature to find the right balance between routing table size, cache and replication storage at
nodes versus lookup latency. The strategy presented in this Section achieves \(O(1)\) lookup on average for networks with standard node
and popularity dynamics. In other words, we allow for node joins, failures, unexpected departures (including malicious
intent) along with changes in service or object popularity. We also allow for "flash crowds", that is, an item, object
or service (such as a table-shard, or certain rows in a table) can quickly gain popularity (as given by standard
Internet virality models \cite{virality_model}).

\begin{itemize}
    \item A node caches or replicates an item with a probability that is proportional to the number of queries it is
        expected to get for that item in the upcoming time interval T.
    \item The difference between a cache and a replica is application specific. For the Database application that the
        network layer hosts, it depends on whether a particular table or a shard is allowed to have write permissions on
        the replica.
    \item When a node caches or replicates an item, it affects the probabilitic demand distribution for the item at
        nodes that are on the routing path which would have gotten the request has this item not been cached.
    \item Thus, by repeating this algorithm iteratively, it would converge to the best caching pattern which would reduce
        lookup times with high probability for all items on the network.
\end{itemize}

\subsection{Node Connectivity protocol}
\label{net:net_proto}

We describe the details of the transport protocol layer of a Picolo node. We call this the Node Connectivity Protocol
layer. This protocol layer is responsible for connecting to a set of peers, maintaining network connectivity under
possibly varying network conditions and allowing for a peer-peer network programing model (as opposed to client-server
programming model which most transport protocols are based off).  The protocol layer should also work with nodes that
are behind Network Address Translators (NATs) \cite{nats}.

When a Picolo node starts for the first time (fresh install), it will query a set of "root" servers, similar to the DNS
architecture of the Internet which is one of the largest decentralized lookup databases in the world \cite{icann_root}. 
These root servers populate the nodes with a list of neighboring nodes and content to bootstrap with. It uses algorithms
outlined in the previous Section to populate its routing table.

As part of downloading the node binary, each download is given a certificate signed by the Picolo root servers. This
certificate is the seed credential that lets the node become part of the network and is presented to other nodes when it
wishes to be part of the core routing layer.

With respect to the network layer semantics for making connections, we shall use either the QUIC protocol or the SPDY
protocol.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{fig/pic_netlayer.png}
  \caption{Comparison of network stacks for Traditional HTTP/2, QUIC and SPDY.}
\end{figure}

QUIC is a relatively new transport protocol designed by Google \cite{quic_sigcomm} that provides encrypted, multi-session transport over UDP
instead of TCP for HTTPS traffic. It replaces the traditional HTTPS stack: HTTP/2, TLS, TCP with an integrated protocol
layer directly over UDP. The key features include reduction in the head-of-line blocking delay and handshake delays to
improve the performance for multi-session connections. However, QUIC is relatively new and the knee-jerk reaction from
firewall manufacturers has been to block it until its fully understood. Also it does not work well with NATs that
provide STUN protocol \cite{stun_protocol} support for drilling holes through their firewalls.  

The network layer would use techniques derived from the STUN protocol to traverse nodes
that are behind a NAT. By having a set of nodes (such as ones with a higher stake) behave as
super-peers, its possible to craft a p2p version of the STUN protocol to enable
other nodes behind a NAT to become part of the network \cite{p2p_nat}.

SPDY (pronounced "SPeeDY"), is a protocol from the Chromium Project at Google \cite{speedy_protocol} to improve network performance for web
pages. It requires changes at both end points for the protocol to work and is able to improve load times by about 64\%
on average. It build upon previous proposals such as the Stream Control Transmission Protocol (SCTP) \cite{sctp}
\cite{sctp_rfc} that allow multiple sessions to coexist on a single TCP connection. SPDY can work well in network
environments where firewalls are biased towards blocking most ports but the HTTP and HTTPS. SPDY allows bi-directional
session initiation to happen and thus is more p2p flavored that other web protocols. It also has multi-session support
over an SSL based secure layer that can utilize certificates as credentials among other methods.

\subsection{Cryptoeconomics and Byzantine behavior}
\label{net:crypto}
How will malicious nodes affect the system and how to mitigate/prevent/recover. Do nodes have incentive to participate in network discovery

\subsection{Analytics and Debug/Fault diagnosis}
\label{net:analytics}
