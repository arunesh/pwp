\section{Network Subsystem} 

The network layer is a fully decentralized, peer-peer overlay routing layer among the nodes participating in the Picolo database network. The most important
goal of the network layer is to help locate content as efficiently and quickly as possible while surviving certain kinds of failure or malicious intent. Peer-peer networking literature refers to this functionality as Decentralized Object Location and Routing (DOLR) \cite{dolr2003}. The network layer focuses on routing messages, such as database queries, read/write requests or other management functions to the respective nodes which can satisfy them. 

The overlay layer can be implemented on top of any datagram network protocol such as UDP or IP. Specifically on the Internet, we realize its implemented on IP or IPv6 protocols.
The peer-to-peer overlay routing infrastructure offers efficient, scalable, location-independent
routing of messages directly to nearby copies of an object or service using only localized resources [TP].

-- Self-repairing.
-- Soft-state based routing.

\subsection{Background}

In this Section, we provide a brief overview of peer-peer systems both in the research and product community that have
influenced the literature over the last 20 years.

Napster \cite{Napster} was one of the first popular service that provided much of the original inspiration for peer-peer
systems although its database was centralized.  DNS is an example of a widely deployed distributed and largely
decentralized key-value database that powers every lookup and interaction on the Internet \cite{Mockapetris_1988}. DNS
relies on special root servers to bootstrap the lookup protocol. The Freenet \cite{freenet_thesis, Clarke_2001} and the
Gnutella \cite{Gnutella} p2p systems were popular in the previous decade for file sharing. Both systems were designed
for sharing of large files over a longer duration of time. Content reliabilty including lookup reliability and network
latency goals were necessary in this enviroment. 

The second generation of peer-peer systems include research driven projects such as Chord \cite{Stoica_2001}, CAN
\cite{Ratnasamy_2001}, Pastry \cite{Rowstron_2001}, Tapestry \cite{tapestry2004} and
Kademlia. Chord along with CAN, Tapestry and Pastry developed the concept of distributed hash tables (DHTs) as a
fundamental mechanism for content-based addressing. They built over the scalability and self-organizing properties of
both FreeNet and Gnutella by providing a definite answer to a lookup query in a bounded number of network hops. In fact,
these protocols are able to locate content within \( O(log N) \) where \(N\) is the number of nodes in the system. From
an 
API perspective, these overlays provide a key-based routing (KBR) interface that supports deterministic routing of
messages to a live node that has the responsiblity for the "value" corresponding to the given key. These systems also
support high level APIs such as Dynamic Object Location and Routing (DOLR) \cite{dolr2003}. Most DHT's use the concept
of consistent hashing to distribute the load evenly among the nodes.

{\em Consistent hashing:}
Typical hashing based schemes do a good job of spreading load through a known, fixed collection of servers. Since the
Blockchain consists of nodes on the Internet which can appear and disappear based on incentives and other criteria, our
assumption is that machines come and go as they crash or are brought into the network. Also,
the information about what nodes are functional propagates slowly through the
network, so that clients may have incompatible “views” of which nodes are available to replicate data. (Note that a
node can also be a client). This makes standard hashing useless since it relies on clients agreeing on which nodes are responsible for serving a particular
page.

Like most hashing schemes, consistent hashing assigns a set of items to buckets so that
each bin receives roughly the same number of items.  Unlike standard hashing schemes, a small change in the bucket set
does not induce a total remapping of items to buckets. In addition, hashing items into slightly different sets of
buckets gives only slightly different assignments of items to buckets. 

Chord uses such hash functions to map nodes and content uniformly to a circular 160-bit
namespace. Chord improves the scalability of consistent hashing by removing the requirement that every node knows about
every other node. Each node only maintains about \(O (log N) \) state information about other nodes in an \( N \) node
network. When nodes join or leave, they require \( O(log^2 N) \) messages to keep the network updated.

When compared to Chord and CAN, Pastry and Tapestry take network distances into account when constructing overlay
topologies. While Chord and CAN use shortest overlay hops and other runtime heuristics, both Tapestry and Pastry
construct locally optimal routing tables to reduce any routing inefficiencies. 

Summary of Pastry, CAN, Tapestry.

Summarize Kademlia.

Applications on top of DHTs, OceanStore and others.

\subsection{Design of the Picolo overlay network}

\subsubsection{Picolo namespace} 
1. Nodes and content map to a single 256-bit space. We can use existing hash functions such as SHA-256 for this purpose.

This space can be segregaged using a namespace identifier, which allows multiple such "naming layers" to co-exist. For example, one way to assign naming layers could be based on a per-application type.

For the rest of this section, the discussion gets isolated to within a single naming layer.

\subsection{Core API}

\subsection{Routing and Lookup}
Explain how a DHT works. Add a layer that takes care of data locality

\subsection{Node Dynamics}

Failures, node departures, node additions.

\subsection{Replication and caching}
Beehive stuff for O(1) lookups for power law queries

\subsection{P2P Connectivity protocol}
Use gossip or some other protocol to maintain network topology, connections

\subsection{Cryptoeconomics}
How will malicious nodes affect the system and how to mitigate/prevent/recover. Do nodes have incentive to participate in network discovery

\subsection{Analytics and Debug/Fault diagnosis}
