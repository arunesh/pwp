\section{Network Subsystem} 
\label{net}
The network layer is a fully decentralized, p2p overlay-based routing layer among the nodes participating in the \textsf{\textsf{Picolo}} network. The most important goal of the network layer is to help locate content as efficiently and quickly as possible while surviving certain kinds of failure or malicious intent. It focuses on delivering messages which have application semantics such as database queries, read/write requests or other management functions to the respective nodes which can satisfy them. In the rest of this section, we use the word object or content to loosely refer to content such as tables, shards, metadata and such, i.e., any
application specific data that is content-addressable and needs to be part of the lookup layer. An item is content addressable if its {\em address} or {\em key} or the primary {\em identifier} is purely a function of its content and does not include parameters related to the location and method of storage. Our p2p overlay routing infrastructure offers efficient, scalable, location-independent routing of messages directly to nearby copies of an object or service using only localized resources.
\newline\newline
\textbf{Location independent routing}: Location independent routing refers to a class of techniques for locating objects
based on content rather than their location, while attempting to find the shortest path possible to reach such objects.
This property is important in our design for \textsf{Picolo} and any decentralized system since it helps reduce the impact of
failures, node departures and Byzantine node behavior (including collusion) since a node does not have explicit {\em
control} over the content it can host or be responsible for. This control gets delegated to the content routing layer,
where it becomes possible to provide some level of hosting/location control to the application
(dapp) developer, user or other policy-based decision processes (such as privacy policies enforced by a government or
organization).  P2p networking literature refers to this functionality as Decentralized Object Location and
Routing (DOLR) \cite{dolr2003}. \textsf{Picolo}'s design offers the following properties which are required of any high performance p2p overlay-based lookup layer:

\begin{itemize}
    \item {\em Determinstic node mapping}: \textsf{Picolo} is able to locate objects anywhere in the network. That is, there
        should be no object in the network that cannot be accessed using the lookup layer. Also, the mapping of objects
        to their location in the network should be the same regardless of where the lookup originates.
    \item {\em Low routing inefficiency}: Routes should have low {\em stretch}. Stretch is the ratio between (network
        level) distance traveled by a query to an object and the minimal network distance from the query to the object.
        Optimal solution would be to always send the query to the nearest copy possible.
    \item {\em Balanced load}: The load should be evenly distributed across the nodes in the network, thus, reducing
        hotspots.
    \item {\em Dynamic membership}: The system allows arrival and departure of nodes while maintaining functionality.
        This is important for handling failures and Byzantine behavior of nodes.
\end{itemize}
In \cref{net:background}, we summarize a wide-range of research on p2p overlays and storage systems
over the last two decades that has influenced our design and thought process. In \cref{net:design} we
present the overall design of the network layer including the \textsf{Picolo} namespace ( \cref{net:namespace}). \cref{net:node_dynamics} presents mechanisms to handle node arrivals and departures into the \textsf{Picolo} overlay network. We discuss route optimizations using caching and replication in \cref{net:replication}.
\newline\newline
The \textsf{Picolo} overlay layer can be implemented on top of any datagram network protocol such as UDP or IP for transport.
While we use IP or IPv6 for the layer-3 connectivity on the Internet, it is possible to use a suite of protocols for the
transport layer depending on the localized network conditions. In \cref{net:net_proto}, we discuss the details of
the transport mechanism used in \textsf{Picolo} for node-to-node communication. Finally, we discuss how responsibility is delegated
to nodes and how trust and governance happens in \cref{net:gov}. For a detailed presentation of the prior research in the area of p2p networks, please read Appendix \cref{appendix:p2p}.

\subsection{Design of the Picolo overlay network}
\label{net:design}

The \textsf{Picolo} overlay network consists of a p2p DHT based lookup for mapping keys to objects, content or services. This
mapping resides inside a namespace, thus functionality gets replicated across namespaces allowing a wide-range of
applications to co-exist. The APIs provided by the network layer are influenced by research from the p2p networking
community and discussed in \cref{net:core_api}.  There is a caching layer that allows for frequently used items to
be propagated closer to the demand endpoints in the network including ability to replicate as needed. A \textsf{Picolo} {\em network node} is a node that participates in the \textsf{Picolo} network overlay functionality such as primary and
secondary routing, functions that allow topology maintenance and related mechanisms. It is possible for a \textsf{Picolo} node to
particpate in the network functionality, the database functionality or both depending on its resources (such as disk
space, internet connectivity).

\subsubsection{Picolo namespace} 
\label{net:namespace}

Both nodes and content have IDs that map to a single 256-bit space using a hash function. This technique is fundamental
to making the routing layer content-addressable. The addressing is hexadecimal based, which is the radix used for the
prefix calculation and lookup in the routing table. This design is similar to how addressing and routing works in IPv6.
The node ID is a hash of its public-key certificate using a SHA-256 hash function. The content ID is a SHA-256 hash of
the application type, database name and version and the table name (and possibly a shard number if needed). As long as
the same consistent method is used to create content IDs, the actual format of the input string doesn't affect
functionality at the network layer (and can thus be determined by the application being used). 
\newline\newline
This space can be segregated using a namespace identifier, which allows multiple such "naming layers" to co-exist. For
example, one way to assign naming layers could be based on a per-application type. The content and node IDs have to be
unique only within the namespace. The default namespace, which is global, is the keyword "default". Each namespace has a separate datastructure and functionality thus, creating an isolated "virtual overlay" on a
per-namespace basis. For the rest of this section, the discussion gets isolated to within a single naming layer, such as
the "default" namespace.

\subsubsection{Core API}
\label{net:core_api}
\textsf{Picolo}'s network layer supports an API similar to a standard p2p overlay network, for a detailed discussion refer
\cite{dolr2003}. The primary goal of the API is to publish and locate objects, content or service identifiers within a given
namespace. All operations that occur in a namespace can be replicated across namespaces as needed. Currenlty, we support the following API methods in a decentralized manner:
\begin{itemize}
    \item \textsf{Publish(namespace, content key, object, credentials)}: Makes the content available on the network, including first a copy on
        the local node. The object could be a database table or a shard. Operations on the object fall within the
        context of the database layer and are discussed in a later section.
    \item \textsf{Unpublish(namespace, content key, credentials)}: Removes the content from the network making it inaccessible.
    \item \textsf{Lookup(namespace, content key, credentials)}: Lookup item with the given content key using the p2p lookup protocol
        discussed in this section. The API returns a node identifier which can be used in the next method to execute a
        remote funationality on it.
    \item \textsf{RemoteCall(namespace, node ID or content key, credentials, method details)}: This executes a given functionality
        on a remote node. If the key is given, it first does a node lookup using the \textsf{Lookup()} method.
\end{itemize}
These methods can be called on a single node. They can also be called using a client SDK which shall be provided. The
client SDK would use the DHT layer to first find "any" node, connect to it and call the above methods. These methods
might use other algorithmic constructs which define core aspects of the routing layer and shall be discussed further in
the following subsections.

\subsubsection{Routing and Lookup}
\label{net:routing}

\textsf{Picolo} uses a prefix-based routing mechanism to find the node for a given content ID. The content ID can be computed by using a combination of the database name, table name and shard ID (if sharding exists, or version information). Optional values such as read/write shards or specific functionality flavors can also be used to "color" the content key (i.e. create variants or replicas etc).
\newline\newline
% TODO Figures:
% network stretch example (nice to have).
% TODO:
% DONE. Describe the routing map. level by level. for a single lookup node. Diagram to illustrate an example routing map.
% DONE. Describe the home node function as hash(content_ID, j) for j = 1 to R_N to generate N home nodes. ContentID = a json
% blob of a 256 bit string and some parameters such as replica size R_N.
% DONE. RouteToDelegates(home_node), describe delegate based routing. Example using a diagram.
% Describe algorithm to construct the neighbor map for a new node. any optimizations.
%   Describe algorithm to notify neighbors.
% Discussion on failures and any optimizations.
% IMP: How are secondary routing links organized ?
\textbf{Routing to a destination ID}:
Given a destination ID, each \textsf{Picolo} node that participates in the routing functionality constructs a routing map, which
is similar to routing tables used in layer-3 systems today. Using these routing maps, routing of a message happens {\em
locally} and {\em incrementally} at each node in the overlay path to the destination. The core routing method is to
route using a prefix-match. This is similar to the Classless InterDomain Routing (CIDR) routing architecture for the Internet
\cite{cidr_rfc}. The routing works as follows. We resolve digits from the most significant to the least (left to right). A node N has a
routing map with multiple levels. The number of levels are equal to the number of digits in the ID space. For a 256 bit
ID with a hexadecimal base, we have 64 digits. At level \textsf{i}, we have nodes that match \textsf{i} digits with the destination ID for \( \textsf{i} \in \textsf{[ 0..63 ]}\). At any given level, the number of entries are equal to the base of the ID space, that is, we have 16 entries, one for
each digit \textsf{[ 0, 1, 2..F ]}. At the \textsf{i} th level, the \textsf{j} th entry is the node with a prefix of \textsf{prefix(ID, i) + j}, where \textsf{prefix(A,j)} represents the prefix of \textsf{A} of length \textsf{j}. As an example, Table \ref{table:routing_map} shows the different levels for the node with ID "691D".  Suppose we wish to route the ID "692B"
given the routing table and we are doing a lookup at level 2. This means we have a prefix match of "69" and are
searching for a node in the column for the digit "2". This happens to be node \(\textsf{C}_\textsf{2}\) and thus the query is then
forwarded there. Similarly, suppose we are asked to route to node "615B". This would be at level 1 since we have a match
for the prefix "6". We would thus route to node \(\textsf{B}_\textsf{1}\) matching the column"1" and row "6".
\begin{table}[h]
\begin{center}
    \renewcommand{\arraystretch}{1.5}
\begin{tabular} {| c | c | c | c | c | c |}
    \hline
    Prefix & 0 & 1 & 2 & .. & F \\
    \hline
    \hline
    None & \(A_0\) & \(A_1\) & \(A_2\) & .. & \(A_F\) \\
    6 & \(B_0\) & \(B_1\) & \(B_2\) & .. & \(B_F\) \\
    69 & \(C_0\) & \(C_1\) & \(C_2\) & .. & \(C_F\) \\
    691 & \(D_0\) & \(D_1\) & \(D_2\) & .. & \(D_F\) \\
    \hline
\end{tabular}
\label{table:routing_map}
\caption{Example of a routing map for node with ID "691D". Suppose we were to route to ID "601D", we would forward this
    to node \(B_0\). Similarly, to route to ID "6911", we would forward to node \(D_1\).}
\end{center}
\end{table}
\textbf{\newline Home Node Set}: The content ID for a given content is computed using a cryptographic function such as SHA-256. The
network design allows the use of any cryptographic hash function with an even distribution of hash values. The content ID 
consists of this hash value, a {\em replica count {\bf r}} and possibly any other parameters bundled as a JSON blob.
As discussed in the previous section, both node IDs and content IDs are confined to the same 256-bit space.  Every content ID
maps to one or more {\em home nodes} that form a {\em home node set}. A home node is the node that is responsible for
either directly hosting the content if it has that capability or has a link to another node that hosts content.
Essentially from the routing layer's perspetive, the routing function ends once we reach a home node. If the replica
count \( r > 1\), then the content maps to a {\em set} of home nodes where each home node is independent from the rest. The function \textsf{HomeNodes(ContentID)} returns a set of home nodes. An implementation of this function using SHA-256 is as follows:
\begin{align}
    X_{id} &= \{X_{256}, r\} \\
    H_X &= HomeNodes(X_{id}) \\
        &= \{ y_i : y_i = \textrm{SHA-256}(X_{256}, j) \quad \forall \quad j = 1 .. r\}
\end{align}
Mapping a content to multiple home nodes is purely for fault tolerance purposes and can be configured by the database
schema (application designer) as needed. For simplicity of the rest of this section, we shall assume that there is a
single home node, i.e., \( r = 1\) or \( |H_X| = 1\) as all the algorithms and concepts easily extend to the case with
multiple home nodes.  In any case, the size of \( H_X \) is expected to be small for any given \(X\).
\newline\newline
In \textsf{Picolo}, nodes can participate in different kinds of functionality such as hosting or routing or both. This
segregation allows certain nodes to either participate in the network routing functionality (and handle that load) or
provide the database functionality or both depending on various factors (such as resource availability). In the case
that a \textsf{Picolo} home node stores a link to another node which hosts the content, such a link is called a {\em content
link}. This link is not part of the routing layer since the routing functionality ends with finding a home node. When a
\textsf{Picolo} node stores a link to another node as a part of the routing map such a link is called a {\em node link}. Thus,
the links stored in a routing map as discussed above are all node links.
\begin{property}[\textsf{Unique Home node set}] The home node set \(H_X\) for a content ID \(X\) must be unique, that is,
    \(H_X\) must generated the exact same set regardless of where it is computed in the network.
\label{property_unique_home}
\end{property}
Note that the example implementation of \textsf{HomeNodes(ContentID)} does satisfy this property. This property is important
to ensure that our decentralized overlay-based routing algorithm as discussed in this section will work in a
deterministic manner regardless of the starting point in the network.
\newline\newline
The goal of the routing protocol is to find a route with low stretch (ideally a stretch of 1.0). Stretch is the ratio
between (network level) distance traveled from (say) node A to node B to the minimal network distance between node A and B.
Since we are routing at the overlay layer, a route on the p2p overlay might result in a suboptimal route at the network
level. Figure \ref{fig:network-stretch} illustrates the concept of network stretch and why a low stretch is important. In this
example, we are routing from node A to F. The p2p overlay route goes from node A to D, then to E, B and finally to F.
This results in an inefficiency as seen at the layer-3 since messages travel the same links multiple times. This
inefficiency is captured by the "stretch" metric as a ratio of the p2p overlay route to the optimal layer-3 route (which
is A to B to F in this example). Since this is at the routing layer which most network messages would take, even small
improvements would result in significant practical performance gains seen at the application level.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.40\textwidth]{fig/network-stretch.png}
  \caption{Example of why a low network stretch is important fo any p2p routing overlay protocol. The solid lines show
    the layer-3 network topology. The solid grey arrow shows the route taken by a p2p overlay routing protocol to route
    from A to F (which is A, D, E, B, F).}
    \label{fig:network-stretch}
\end{figure}

\textbf{\newline Secondary routing layer for low-stretch}: Network nodes in \textsf{Picolo} have two types of routing layers. The first is
called a {\em principal} route layer which defines the core neighbor links that connect the node to its neighbors. The
Principal or primary route layer uses the route map (as discussed above) that uses a prefix-based routing algorithm. The
goal of the principal routing layer is to maintain a highly reliable foundational network topology. Issues with these
links might cause the node to be marked as having serious failures (and thus trigger failure mode operation semantics).
The second set of links are direct links to specific nodes and are maintained as a hash table of \textsf{<Node ID, IP Address>}
mappings. These are used for optimizations, such as to store direct content links to popular items or to cache links to popular home
nodes, etc. We call these the {\em cache links} or {\em secondary links}. We shall discuss the distributed caching
algorithm separately in \cref{net:replication} which will provide probabilistic guarantees of a lookup cost of
\(O(1)\) for most queries thus reducing the routing stretch.

\textbf{\newline Explicit soft-state publishing:} Nodes in \textsf{Picolo} send periodic {\em heartbeat} messages to other nodes through the
p2p overlay network as given by their routing tables. This heatbeat message includes any content links or content IDs
that the nodes are hosting. These messages are routed along the {\em principal} routing links which every node maintains
for two reasons: Firstly, these messages are not latency critical and thus don't need to use the secondary layer. They
serve to validate and refresh the network metrics of the primary links. They help discover "holes" or link/node
failures. Secondly, these heartbeat messages also help deposit content links at various intermediate nodes that help
route them. The content links and latency metrics expire according to a timeout and are refreshed with such messages.
This operation of advertising content with an automatic expiry is called {\em soft-state publishing}. \textsf{Picolo} also
supports targeted re-publishing of content such as when a node comes back online after a disruption or when a node
leaves/joins the network.

\textbf{\newline Query routing:} Lookup or querying for a specific content ID (or a set) happens in the following manner. A client
that wishes to make such a query connects to one of the network nodes in \textsf{Picolo}. The network node then computes
\textsf{HomeNodes(contentID)} for the requested content. The lookup or query request is then forwarded to one (or more)
of the home nodes from this set. In the process of this lookup, if at any stage, multiple node paths are encountered,
then the query can be forwarded along all paths (to minimize latency but incur a slightly higher cost) or can be
forwarded along the path with the best latency. These tradeoffs can be exposed at the \textsf{HomeNodes(contentID)} API
call to allow the application developer to make such choices.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.55\textwidth]{fig/routing-eg.png}
  \caption{Example of routing a query for the ID "5824". The query originates at node "691D" in the above illustration.
    The solid black lines show the principal routing map. The dashed lines show the cached links. The think grey curved
    arrow shows the eventual path taken by the query.}
    \label{fig:routing-eg}
\end{figure}

Figure \ref{fig:routing-eg} shows an example of routing a query for a content ID "5824" originating at node with ID
"691D". The dotted arrows show the cached content links from secondary or cache routing network. The primary or
principal links are shown in solid arrow. In this example, node "691D" routes the query for "5824" to the node "5146"
first as a level 0 match in its principal routing map. At this point, the first digit "5" is match and the node "5146"
has a principal route to "5891" (matches "58") and then to "5823" (matches "582") which is the home node for the content
ID "5824". "5823" then routes the query to the storage node which hosts the content. This would be the route that a
query could take if it were to follow the principal routes in the routing map.

The secondary routes are also shown in the Figure as dotted lines. In the example shown, node "5146" has a cached link
to the storage node for the content ID "5824" and thus directly routes the message to that node. Cached links can thus
optimize the routing and reduce the network stretch by a significant factor.  In practice, our route caching algorithm will
ensure that most lookups and queries can be satisfied using the cache routing network, so that we can get an \(O(1)\) lookup
performance in the expected/average case.

%The example the lookup operation
%for the content ID YYY originating at two different points in the network A and B. The query at A utilizes the principal
%links and finds the home node for the content ID YYY. The query at B ends up hitting a node C which has a cached link that
%directly routes it to the home node for YYY, thus saving a few network hops.

\begin{theorem}
\textsf{Picolo}'s network layer can perform location independent routing given Property \ref{property_unique_home}.
\end{theorem}

The proof for this theorem follows from two facts. First, for a content ID \(X\), the function \(H_X = \textsf{HomeNodes(X)}\) returns
the same set of nodes regardless of which network node this is executed on, thus, proving that the result is purely a function
of the content ID \(X\). Secondly, we note that the routing protcol requires home nodes (either directly hosting content or content links)
to periodically send heartbeat messages through the principal links to the rest of the network. This ensures that any node in the 
network will have the routing layer capabilty to reach atleast one node in this set \(H_X\), thus proving the
"routability" of the home node set \(H_X\). This proves the Theorem. 

\begin{theorem}[\textsf{Fault Tolerance}]
   
    Let \(X\) be any content ID and let its set of home nodes be given by \( H_X = \textsf{HomeNodes(X)} \). Also, let \(S_n\) be
    the set of all nodes (of cardinality \(n\)) which form the network that satisfy the above properties and Theorems.
    Now, if \( |H_x| > 1 \) and \( \forall a, b \in S_n, P(a|b) = P(a) \) where \( P(a) = \) probability of node \(a \in
    H_x \), then \(H_x\) has fault tolerance with relabilty of \( | H_x | \).

\end{theorem}

The proof of this Theorem can be seen in the following manner. First, the Theorem \ref{property_unique_home} states that
\textsf{Picolo} can do location independent routing for any node \(y \in S_n\) where \(S_n\) is the set of nodes in the network.
Now, for a content ID \(X\) if the probability of any node \(z \in H_X\) is uncorrelated with any other nodes (they are
independent) then by Theorem \ref{property_unique_home} they can be independently routed by the network. Now, the fault
tolerance property follows from the observation that the only remaining way faults (network or node failures) can be
correlated is by attempting a hash-prediction (reverse hash attacks) on the cryptographic hash function used in the
\textsf{HomeNodes(X)} function, which has been proven to be computationally hard for our choice of SHA-256.

\subsubsection{Delegate Routing}

There is one practical aspect of routing in a p2p overlay like \textsf{Picolo} that we have not addressed in order to
simpify the discussion thus far.  This has to do with whether home nodes will actually exist in a sparse network space.
We discuss this issue and present a solution that is a drop-in replacement for the \textsf{HomeNodes(X)} function and
preserves the above Theorems and properties.

Node and content IDs in \textsf{Picolo} map to a relatively large space, i.e. \(2^{256}\) in size. Let \( H_X =
\textsf{HomeNodes(X)}\) for content ID X. Given the large ID space and relatively sparse nature of the network nodes (a
thousand or a million nodes out of \(2^{256}\), it is unlikely that nodes in \(H_X\) would have an exact match in the
network. (this is expected with any content based DHTs routing layer \cite{Stoica_2001, Rowstron_2001}).
Nevertheless, we route to each node \(y \in H_X\) as if it exists. During this process we might encounter empty neighbor
links on the way. In such cases, the algorithm selects an alternate link but does it in a way that is deterministic
across the whole network. Routing will terminate when we reach a node which is the best match and the lookup cannot be
routed further. This node then, becomes the {\em Delegate} for the subject node \(y\). We call this {\em Delegate
Routing} where we route the query to each node \(y \in H_X \) such that it gets mapped to a node that is a {\em
delegate} for the home node with ID \(y\).

In the routing table, an entry at a level \(l\) for an digit \(k\) will be empty only if no such nodes exist in the
network that have a match. Thus, the same entry will be empty for all nodes in the network. As a result, regardless of
where the lookup query originates for a content ID \(X\), it will always compute to the same delegate node \(y\).
Delegate routing thus becomes a determinstic routing function computed by the network in a distributed manner. It is
important to note that this delegate resolution is the same regardless of where the function computation originates in the network.

Now, we can replace the function \textsf{HomeNodes(X)} with a new function \(H_X = \textsf{DelegateNodes(X)}\) that automatically returns a
set \(H_X\) such that each node in \(H_X\) exists in the network. All the analysis, properties and Theorems discussed
above extend directly to the \textsf{DelegateNodes(X)} as a drop-in replacement for the \textsf{HomeNodes(X)} function.

% TODO mark this as a property ?
% We solve this problem by mapping each node \(y \in H_x \) to a \(Delegate(y)\) which is a node that exists on the network.

\subsection{Node Dynamics}
\label{net:node_dynamics}

We discuss how the network functions when new nodes join or existing nodes leave the network including unexpected node
departures.

\subsubsection{Node insertion}

In this section, we discuss how a new node can join the network and create a routing map for both the principal layer
and the secondary layer. The routing map has to be created so that the network properties and key Theorems hold.

Node insertion happens in the following manner. A new node \(N\) first creates its 256 bit ID,
say, \(N_{id}\), using any credentials it has. Credentials can include node identifier information, owernship
certificate information and such. Next, \( N \) connects to the core nodes that are available through DNS mechanisms or
other bootstrapping methods. It uses a special API call requesting that \(N\) be inserted into the network. This API
call helps \(N\) become part of the network by building its principal routing map correctly and informing other nodes
that it is available.

\(N\) computes its principal routing maps in the following manner. It uses the Delegate routing function to find the set of delegate
nodes to route to the content ID \(N_{id}\). However, at each hop we use the information from the nodes being contacted
to construct the neighbor map.
\newline\newline
\textbf{New node notification}: The final step is to inform the other nodes of \(N\). This is done by going through the
nodes at each level in the routing map that \(N\) has constructed and sending them an explicit heartbeat message. This
includes any nodes that are delegates for \(N\). This will ensure that new queries will start getting routed to \(N\).

\subsubsection{Failures and node departures}

Nodes can depart the \textsf{Picolo} overlay network in two ways: First, is the case of a graceful departure, where the departing
node send out an explicit notification through the network informing of the departure. This allows the network to adapt
before the node leaves, by updating the routing tables and moving content out of the departing node if necessary.

Second is the case of a sudden departure such as a node getting shutdown, failing for various reasons or getting
disconnected from the network due to Internet connectivity issues. A node with a degraded Internet connection can also
be treated under this scenario. The basic mechanism to recover from such failures is using the heartbeat advertisements
as discussed previously. Each link on the overlay network includes heartbeat messages which help update the latency
metrics and also result in re-publishing or updating the content list hosted on a node. Upon missing a certain number of
messages from a node suspected of failure, neighboring nodes and request an explicit update from such a node. Upon
timeout, the neighboring nodes arrive at a consensus that the suspected node has failed and thus update their routing
tables accordingly by initiating the departure API calls as for the first case.

\subsection{Replication and caching}
\label{net:replication}
Structured p2p distributed hash tables can implement lookups in \( O(log N)\) hops. Since each hop could add between 100-500
ms of latency, for a network of 10K to 100K nodes this computes to about 4-5 seconds for a lookup (or more). Thus, we
require a caching and replication strategy to reduce this cost especially for popular items. The discussion in this
section applies to any content or service thats is made available through the p2p system and thus extends beyond \textsf{Picolo}.
\newline\newline
There has been significant prior work on caching and replicating strategies for p2p overlay networks. Methods such
as Beehive \cite{beehive} provide a closed-form replication algorithm based on a derived equation that guarantees
\(O(1)\) lookup but requires full knowledge of the network, such as number of nodes and popularity values for each data
item. While this system might be good for analysis and benchmark purposes, its implementation is not pratical for
\textsf{Picolo}. Kelips \cite{kelips} is a probabilistic algorithm that also provides \(O(1)\) lookup performance (in the
expected case) by dividing the network into \(O(\sqrt(N))\) affinity groups each of \(\sqrt(N)\) size. They use a gossip protocol to replicate content to all nodes within an affinity group. Work by Gupta et al \cite{one_hop_lookup}
explore the tradeoff between routing table size and lookup latency. They offer a guaranteed \(O(1)\) lookup by
maintaining routes to each and every node in the network. Farsite \cite{farsite} provides a better ttradeoff by using
routing tables of \(O(dn^{1/3})\) size to route within \(O(d)\) hops. Other p2p applications such as PAST \cite{past} and CFS \cite{cfs} use fixed size caches on intermediate nodes to
cache the objects being queried. While they are unable to provide closed-form analytic bounds on query time, their
average case performance is reasonably good.
\newline\newline
We draw on the above literature to find the right balance between routing table size, cache and replication storage at
nodes versus lookup latency. The strategy presented in this section achieves \(O(1)\) lookup on average for networks
with standard node and popularity dynamics. In other words, we allow for node joins, failures, unexpected departures
(including malicious intent) along with changes in service or object popularity. We also allow for "flash crowds", that
is, an item, object or service (such as a table-shard, or certain rows in a table) can quickly gain popularity (as given
by standard Internet virality models \cite{virality_model}).
\begin{itemize}
    \item A node caches or replicates an item with a probability that is proportional to the number of queries it is
        expected to get for that item in the upcoming time interval T.
    \item The difference between a cache and a replica is application specific. For the database application that the
        network layer hosts, it depends on whether a particular table or a shard is allowed to have write permissions on
        the replica.
    \item When a node caches or replicates an item, it affects the probabilitic demand distribution for the item at
        nodes that are on the routing path which would have gotten the request has this item not been cached.
    \item Thus, by repeating this algorithm iteratively, it would converge to the best caching pattern which would reduce
        lookup times with high probability for all items on the network.
\end{itemize}
Each node maintains these cached links as a part of the secondary or "cache" routing layer which is essentially a lookup
map of \textsf{<Node ID, IP Address>} pairs. It is possible for nodes to maintain relatively large caches since they are only
limited by the memory and node dynamic characteristics of the network. Cached links that use the methodologies discussed
above can significantly reduce lookup latencies. Also the downside of a stale cache link is quite low, since any node
can always initiate a principal routing layer based lookup procedure.

\subsection{Node Connectivity protocol}
\label{net:net_proto}

We describe the details of the transport protocol layer of a \textsf{Picolo} node. We call this the Node Connectivity Protocol
layer. This protocol layer is responsible for connecting to a set of peers, maintaining network connectivity under
possibly varying network conditions and allowing for a p2p network programing model (as opposed to client-server
programming model which most transport protocols are based off).  The protocol layer should also work with nodes that
are behind Network Address Translators (NATs).
\newline\newline
When a \textsf{Picolo} node starts for the first time (fresh install), it will query a set of "root" servers, similar to the DNS
architecture of the Internet which is one of the largest decentralized lookup databases in the world \cite{icann_root}. 
These root servers populate the nodes with a list of neighboring nodes and content to bootstrap with. It uses algorithms
outlined in the previous section to populate its routing table. As part of downloading the node binary, each download is given a certificate signed by the \textsf{Picolo} root servers. This
certificate is the seed credential that lets the node become part of the network and is presented to other nodes when it
wishes to be part of the core routing layer. With respect to the network layer semantics for making connections, we shall use either the QUIC protocol or the SPDY
protocol.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/pic_netlayer.png}
  \caption{Comparison of network stacks for Traditional HTTP/2, QUIC and SPDY.}
\end{figure}
\newline\newline
QUIC is a relatively new transport protocol designed by Google \cite{quic_sigcomm} that provides encrypted, multi-session transport over UDP
instead of TCP for HTTPS traffic. It replaces the traditional HTTPS stack: HTTP/2, TLS, TCP with an integrated protocol
layer directly over UDP. The key features include reduction in the head-of-line blocking delay and handshake delays to
improve the performance for multi-session connections. However, QUIC is relatively new and the knee-jerk reaction from
firewall manufacturers has been to block it until its fully understood. Also it does not work well with NATs that
provide STUN protocol \cite{stun_protocol} support for drilling holes through their firewalls.  The network layer would use techniques derived from the STUN protocol to traverse nodes
that are behind a NAT. By having a set of nodes (such as ones with a higher stake) behave as
super-peers, its possible to craft a p2p version of the STUN protocol to enable
other nodes behind a NAT to become part of the network \cite{p2p_nat}.
\newline\newline
SPDY (pronounced "SPeeDY"), is a protocol from the Chromium Project at Google \cite{speedy_protocol} to improve network performance for web
pages. It requires changes at both end points for the protocol to work and is able to improve load times by about 64\%
on average. It builds upon previous proposals such as the Stream Control Transmission Protocol (SCTP)
\cite{sctp_rfc} that allows multiple sessions to coexist on a single TCP connection. SPDY can work well in network
environments where firewalls are biased towards blocking most ports but the HTTP and HTTPS. SPDY allows bi-directional
session initiation to happen and thus is more p2p flavored that other web protocols. It also has multi-session support
over an SSL based secure layer that can utilize certificates as credentials among other methods.

%\subsection{Cryptoeconomics and Byzantine behavior}
%\label{net:crypto}
%How will malicious nodes affect the system and how to mitigate/prevent/recover. Do nodes have incentive to participate in network discovery

\subsection{Trust and Governance}
\label{net:gov}

\subsubsection{Analytics and Debug/Fault diagnosis}
Nodes maintain local analytics which are periodically updated to a special distributed system-level analytics
database. Analytics include counters for table queries, statistics for various operations (CRUD), latency numbers, route
changes and similar network statistics. The analytics database is a special permissioned database that only entities
(developers, institutions) with the right credentials can modify. A similar but separate database is used for fault diagnosis, collecting data on Byzantine failures, crash logs and
unexpected node departures. A third database is purely used for logs, both at the network level and the database level. Both databases are readable by all nodes who are part of the network. These databases provide the basic mechanism for
establishing trust and stake for nodes in the network.

\subsubsection{Trust ladder}
While we haven't created a detailed specification for how trust and governance work in the network, the basic principles
are the following:
\begin{itemize}
    \item New nodes that obtain a fresh certificate, start at the lowest level of trust. They are able to participate in
        read-only functionalities of the network, such as basic routing, providing read-only replicas and such.
    \item Using the network, debug/ fault and analytics logs, the network is able to elevate the trust level of a node.
        This can be seen as a form of stake. The network can also allow nodes that place a security deposit to start at
        a certain level of trust.
    \item Nodes at a higher level of trust gain access to great amounts of functionality. For example, the ability to
        change the principal routes, or be able to bootstrap new nodes into the network requires a higher level of trust
        or stake.
    \item The highest level of trust includes nodes that have the ability to update node binaries, issue certificates,
        grant access to other nodes and similar acts of governance. While it might be possible for regular nodes to
        enter this level through a combination of service-based trust and stake, the membership might be managed by a
        council of network admins or developers.
\end{itemize}
